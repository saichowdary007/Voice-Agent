import google.generativeai as genai
from src.config import GEMINI_API_KEY, MAX_CONTEXT_TOKENS

class LLM:
    """
    Handles interaction with the Large Language Model (Google Gemini).
    """
    def __init__(self):
        if not GEMINI_API_KEY:
            raise ValueError("GEMINI_API_KEY is not set. Please check your .env file.")
        
        genai.configure(api_key=GEMINI_API_KEY)
        
        generation_config = {
            "temperature": 0.7,
            "top_p": 1,
            "top_k": 1,
            "max_output_tokens": 256,
        }

        safety_settings = [
            {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
            {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
            {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
            {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
        ]

        self.model = genai.GenerativeModel(
            model_name="gemini-1.5-flash", # Using flash as per PRD for speed
            generation_config=generation_config,
            safety_settings=safety_settings
        )

    def generate_response(self, user_text: str, conversation_history: list, language: str = 'en') -> str:
        """
        Generates a response from the LLM based on the user's input and conversation history.

        Args:
            user_text: The transcribed text from the user.
            conversation_history: A list of past messages from the ConversationManager.
            language: The detected language code of the user's text.

        Returns:
            The text response generated by the LLM.
        """
        try:
            # The history is now managed by the ConversationManager and passed in.
            # It should already be in the correct format for the model.
            
            # Combine the retrieved history with the current user text
            full_context = conversation_history + [{"role": "user", "parts": [user_text]}]

            response = self.model.generate_content(full_context)
            
            model_response_text = response.text
            return model_response_text
        except Exception as e:
            print(f"Error generating response from LLM: {e}")
            return "I'm sorry, I'm having trouble connecting to my brain right now."

if __name__ == '__main__':
    # The example usage needs to be updated as the class now depends on
    # an external ConversationManager to provide history.
    # This direct example is no longer as meaningful without that context.
    print("--- LLM Example (now context-dependent) ---")
    
    try:
        llm = LLM()
        # This example now only shows a single-turn conversation
        # as we don't have a ConversationManager instance here.
        print("\nUser: Hello, what can you do?")
        response = llm.generate_response(
            user_text="Hello, what can you do?", 
            conversation_history=[], 
            language='en'
        )
        print(f"Assistant: {response}")

    except ValueError as e:
        print(f"\nCould not run example: {e}")
    
    print("-------------------------------------------\n")
